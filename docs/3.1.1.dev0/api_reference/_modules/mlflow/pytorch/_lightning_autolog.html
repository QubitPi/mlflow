

<!DOCTYPE html>
<!-- source: docs/source/_modules/mlflow/pytorch/_lightning_autolog -->
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mlflow.pytorch._lightning_autolog</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/_modules/mlflow/pytorch/_lightning_autolog.html">
  
  
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
  

  

  
    
        <!-- Google Tag Manager -->
        <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
        new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
        'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer',"GTM-5J249Z5D");</script>
        <!-- End Google Tag Manager -->
    
  
  
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=AW-16857946923"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'AW-16857946923');
  </script>
  <!-- Eng gtag -->

  

  
  <meta name="docsearch:docusaurus_tag" content="docs-default-current" data-rh="true">
  <meta name="docusaurus_tag" content="docs-default-current" data-rh="true">

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../../../genindex.html"/>
        <link rel="search" title="Search" href="../../../search.html"/>
    <link rel="top" title="MLflow 3.1.1.dev0 documentation" href="../../../index.html"/>
        <link rel="up" title="mlflow.pytorch" href="../pytorch.html"/> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../../../_static/documentation_options.js"></script>
<script type="text/javascript" src="../../../_static/jquery.js"></script>
<script type="text/javascript" src="../../../_static/underscore.js"></script>
<script type="text/javascript" src="../../../_static/doctools.js"></script>
<script type="text/javascript" src="../../../_static/tabs.js"></script>
<script type="text/javascript" src="../../../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="../../../_static/runllm.js"></script>

<body class="wy-body-for-nav" role="document">
  
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5J249Z5D"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <div class="header-container">
  <style scoped>
    .header-container {
      display: flex;
      flex-direction: row;
      align-items: center;
      justify-content: space-between;
      padding: 8px 16px;

      background-color: #fff;
      box-shadow: 0 1px 2px 0 #0000001a;
    }

    .logo-container {
      display: flex;
      gap: 12px;
      flex-direction: row;
      white-space: nowrap;
      align-items: center;
      justify-content: center;
    }

    a:hover {
      text-decoration: none;
      color: #0194e2;
    }
  </style>
  <div class="logo-container">
    <i
      data-toggle="wy-nav-top"
      class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"
    ></i>
    <a href="../../../index.html" class="wy-nav-top-logo">
      <img
        src="../../../_static/MLflow-logo-final-black.png"
        alt="MLflow"
      />
    </a>
    <a
      style="overflow: hidden; text-overflow: ellipsis"
      class="header-link"
      href="/docs/latest"
      >Main Docs</a
    >
    <span style="overflow: hidden; text-overflow: ellipsis" class="header-link"
      >API Documentation</span
    >
  </div>
  <span class="header-link">3.1.1.dev0</span>
</div>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../../index.html">Home</a>
    

    
      

      
        <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../auth/python-api.html">MLflow Authentication Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../auth/rest-api.html">MLflow Authentication REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rest-api.html">REST API</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../../index.html">Module code</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
        <li><a href="../pytorch.html">mlflow.pytorch</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>mlflow.pytorch._lightning_autolog</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/_modules/mlflow/pytorch/_lightning_autolog" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  <h1>Source code for mlflow.pytorch._lightning_autolog</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">packaging.version</span><span class="w"> </span><span class="kn">import</span> <span class="n">Version</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">mlflow.pytorch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.exceptions</span><span class="w"> </span><span class="kn">import</span> <span class="n">MlflowException</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.ml_package_versions</span><span class="w"> </span><span class="kn">import</span> <span class="n">_ML_PACKAGE_VERSIONS</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.utils.autologging_utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">BatchMetricsLogger</span><span class="p">,</span>
    <span class="n">ExceptionSafeAbstractClass</span><span class="p">,</span>
    <span class="n">MlflowAutologgingQueueingClient</span><span class="p">,</span>
    <span class="n">disable_autologging</span><span class="p">,</span>
    <span class="n">get_autologging_config</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlflow.utils.checkpoint_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">MlflowModelCheckpointCallbackBase</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
<span class="n">MIN_REQ_VERSION</span> <span class="o">=</span> <span class="n">Version</span><span class="p">(</span><span class="n">_ML_PACKAGE_VERSIONS</span><span class="p">[</span><span class="s2">&quot;pytorch-lightning&quot;</span><span class="p">][</span><span class="s2">&quot;autologging&quot;</span><span class="p">][</span><span class="s2">&quot;minimum&quot;</span><span class="p">])</span>
<span class="n">MAX_REQ_VERSION</span> <span class="o">=</span> <span class="n">Version</span><span class="p">(</span><span class="n">_ML_PACKAGE_VERSIONS</span><span class="p">[</span><span class="s2">&quot;pytorch-lightning&quot;</span><span class="p">][</span><span class="s2">&quot;autologging&quot;</span><span class="p">][</span><span class="s2">&quot;maximum&quot;</span><span class="p">])</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">pytorch_lightning</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pl</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pytorch_lightning.utilities</span><span class="w"> </span><span class="kn">import</span> <span class="n">rank_zero_only</span>

<span class="c1"># The following are the downsides of using PyTorch Lightning&#39;s built-in MlflowLogger.</span>
<span class="c1"># 1. MlflowLogger doesn&#39;t provide a mechanism to store an entire model into mlflow.</span>
<span class="c1">#    Only model checkpoint is saved.</span>
<span class="c1"># 2. For storing the model into mlflow `mlflow.pytorch` library is used</span>
<span class="c1"># and the library expects `mlflow` object to be instantiated.</span>
<span class="c1"># In case of MlflowLogger, Run management is completely controlled by the class and</span>
<span class="c1"># hence mlflow object needs to be reinstantiated by setting</span>
<span class="c1"># tracking uri, experiment_id and run_id which may lead to a race condition.</span>
<span class="c1"># TODO: Replace __MlflowPLCallback with Pytorch Lightning&#39;s built-in MlflowLogger</span>
<span class="c1"># once the above mentioned issues have been addressed</span>

<span class="n">_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">_pl_version</span> <span class="o">=</span> <span class="n">Version</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="k">if</span> <span class="n">_pl_version</span> <span class="o">&lt;</span> <span class="n">Version</span><span class="p">(</span><span class="s2">&quot;1.5.0&quot;</span><span class="p">):</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">pytorch_lightning.core.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelSummary</span>
<span class="k">else</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">pytorch_lightning.utilities.model_summary</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelSummary</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_optimizer_name</span><span class="p">(</span><span class="n">optimizer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    In pytorch-lightning 1.1.0, `LightningOptimizer` was introduced:</span>
<span class="sd">    https://github.com/PyTorchLightning/pytorch-lightning/pull/4658</span>

<span class="sd">    If a user sets `enable_pl_optimizer` to True when instantiating a `Trainer` object,</span>
<span class="sd">    each optimizer will be wrapped by `LightningOptimizer`:</span>
<span class="sd">    https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.html</span>
<span class="sd">    #pytorch_lightning.trainer.trainer.Trainer.params.enable_pl_optimizer</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">Version</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">Version</span><span class="p">(</span><span class="s2">&quot;1.1.0&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">optimizer</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">pytorch_lightning.core.optimizer</span><span class="w"> </span><span class="kn">import</span> <span class="n">LightningOptimizer</span>

        <span class="k">return</span> <span class="p">(</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">LightningOptimizer</span><span class="p">)</span>
            <span class="k">else</span> <span class="n">optimizer</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">__MlflowPLCallback</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">Callback</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">ExceptionSafeAbstractClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Callback for auto-logging metrics and parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">client</span><span class="p">,</span> <span class="n">metrics_logger</span><span class="p">,</span> <span class="n">run_id</span><span class="p">,</span> <span class="n">log_models</span><span class="p">,</span> <span class="n">log_every_n_epoch</span><span class="p">,</span> <span class="n">log_every_n_step</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">log_every_n_step</span> <span class="ow">and</span> <span class="n">_pl_version</span> <span class="o">&lt;</span> <span class="n">Version</span><span class="p">(</span><span class="s2">&quot;1.1.0&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="s2">&quot;log_every_n_step is only supported for PyTorch-Lightning &gt;= 1.1.0&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span> <span class="o">=</span> <span class="n">client</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics_logger</span> <span class="o">=</span> <span class="n">metrics_logger</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run_id</span> <span class="o">=</span> <span class="n">run_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_models</span> <span class="o">=</span> <span class="n">log_models</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_every_n_epoch</span> <span class="o">=</span> <span class="n">log_every_n_epoch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_every_n_step</span> <span class="o">=</span> <span class="n">log_every_n_step</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_global_steps_per_training_step</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="c1"># Sets for tracking which metrics are logged on steps and which are logged on epochs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_step_metrics</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_epoch_metrics</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_log_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">metric_items</span><span class="p">):</span>
        <span class="c1"># pytorch-lightning runs a few steps of validation in the beginning of training</span>
        <span class="c1"># as a sanity check to catch bugs without having to wait for the training routine</span>
        <span class="c1"># to complete. During this check, we should skip logging metrics.</span>
        <span class="c1"># https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#num-sanity-val-steps</span>
        <span class="n">sanity_checking</span> <span class="o">=</span> <span class="p">(</span>
            <span class="c1"># `running_sanity_check` has been renamed to `sanity_checking`:</span>
            <span class="c1"># https://github.com/PyTorchLightning/pytorch-lightning/pull/9209</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">sanity_checking</span>
            <span class="k">if</span> <span class="n">Version</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">Version</span><span class="p">(</span><span class="s2">&quot;1.4.5&quot;</span><span class="p">)</span>
            <span class="k">else</span> <span class="n">trainer</span><span class="o">.</span><span class="n">running_sanity_check</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">sanity_checking</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># Cast metric value as  float before passing into logger.</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">metric_items</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics_logger</span><span class="o">.</span><span class="n">record_metrics</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_log_epoch_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
        <span class="c1"># `trainer.callback_metrics` contains both training and validation metrics</span>
        <span class="c1"># and includes metrics logged on steps and epochs.</span>
        <span class="c1"># If we have logged any metrics on a step basis in mlflow, we exclude these from the</span>
        <span class="c1"># epoch level metrics to prevent mixing epoch and step based values.</span>
        <span class="n">metric_items</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span> <span class="ow">in</span> <span class="n">trainer</span><span class="o">.</span><span class="n">callback_metrics</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_step_metrics</span>
        <span class="p">]</span>
        <span class="c1"># Record which metrics are logged on epochs, so we don&#39;t try to log these on steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_epoch_metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">name</span> <span class="k">for</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="n">metric_items</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">pl_module</span><span class="o">.</span><span class="n">current_epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_every_n_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_metrics</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="o">.</span><span class="n">current_epoch</span><span class="p">,</span> <span class="n">metric_items</span><span class="p">)</span>

    <span class="n">_pl_version</span> <span class="o">=</span> <span class="n">Version</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>

    <span class="c1"># In pytorch-lightning &gt;= 1.4.0, validation is run inside the training epoch and</span>
    <span class="c1"># `trainer.callback_metrics` contains both training and validation metrics of the</span>
    <span class="c1"># current training epoch when `on_train_epoch_end` is called:</span>
    <span class="c1"># https://github.com/PyTorchLightning/pytorch-lightning/pull/7357</span>
    <span class="k">if</span> <span class="n">_pl_version</span> <span class="o">&gt;=</span> <span class="n">Version</span><span class="p">(</span><span class="s2">&quot;1.4.0dev&quot;</span><span class="p">):</span>

        <span class="nd">@rank_zero_only</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">on_train_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_epoch_metrics</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">)</span>

    <span class="c1"># In pytorch-lightning &gt;= 1.2.0, logging metrics in `on_epoch_end` results in duplicate</span>
    <span class="c1"># metrics records because `on_epoch_end` is called after both train and validation</span>
    <span class="c1"># epochs (related PR: https://github.com/PyTorchLightning/pytorch-lightning/pull/5986)</span>
    <span class="c1"># As a workaround, use `on_train_epoch_end` and `on_validation_epoch_end` instead</span>
    <span class="c1"># in pytorch-lightning &gt;= 1.2.0.</span>
    <span class="k">elif</span> <span class="n">_pl_version</span> <span class="o">&gt;=</span> <span class="n">Version</span><span class="p">(</span><span class="s2">&quot;1.2.0&quot;</span><span class="p">):</span>
        <span class="c1"># NB: Override `on_train_epoch_end` with an additional `*args` parameter for</span>
        <span class="c1"># compatibility with versions of pytorch-lightning &lt;= 1.2.0, which required an</span>
        <span class="c1"># `outputs` argument that was not used and is no longer defined in</span>
        <span class="c1"># pytorch-lightning &gt;= 1.3.0</span>

        <span class="nd">@rank_zero_only</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">on_train_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Log loss and other metrics values after each train epoch</span>

<span class="sd">            Args:</span>
<span class="sd">                trainer: pytorch lightning trainer instance</span>
<span class="sd">                pl_module: pytorch lightning base module</span>
<span class="sd">                args: additional positional arguments</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="c1"># If validation loop is enabled (meaning `validation_step` is overridden),</span>
            <span class="c1"># log metrics in `on_validaion_epoch_end` to avoid logging the same metrics</span>
            <span class="c1"># records twice</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">trainer</span><span class="o">.</span><span class="n">enable_validation</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_log_epoch_metrics</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">)</span>

        <span class="nd">@rank_zero_only</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">on_validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Log loss and other metrics values after each validation epoch</span>

<span class="sd">            Args:</span>
<span class="sd">                trainer: pytorch lightning trainer instance</span>
<span class="sd">                pl_module: pytorch lightning base module</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_epoch_metrics</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>

        <span class="nd">@rank_zero_only</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">on_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Log loss and other metrics values after each epoch</span>

<span class="sd">            Args:</span>
<span class="sd">                trainer: pytorch lightning trainer instance</span>
<span class="sd">                pl_module: pytorch lightning base module</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_epoch_metrics</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">)</span>

    <span class="nd">@rank_zero_only</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Log metric values after each step</span>

<span class="sd">        Args:</span>
<span class="sd">            trainer: pytorch lightning trainer instance</span>
<span class="sd">            pl_module: pytorch lightning base module</span>
<span class="sd">            args: additional positional arguments</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_every_n_step</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="c1"># When logging at the end of a batch step, we only want to log metrics that are logged</span>
        <span class="c1"># on steps. For forked metrics (metrics logged on both steps and epochs), we exclude the</span>
        <span class="c1"># metric with the non-forked name (eg. &quot;loss&quot; when we have &quot;loss&quot;, &quot;loss_step&quot; and</span>
        <span class="c1"># &quot;loss_epoch&quot;) so that this is only logged on epochs. We also record which metrics</span>
        <span class="c1"># we&#39;ve logged per step, so we can later exclude these from metrics logged on epochs.</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="n">_get_step_metrics</span><span class="p">(</span><span class="n">trainer</span><span class="p">)</span>
        <span class="n">metric_items</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span> <span class="ow">in</span> <span class="n">metrics</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epoch_metrics</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">_step&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">metrics</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_step_metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">name</span> <span class="k">for</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="n">metric_items</span><span class="p">)</span>
        <span class="n">step</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span>
        <span class="k">if</span> <span class="p">((</span><span class="n">step</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_steps_per_training_step</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_every_n_step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_metrics</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">metric_items</span><span class="p">)</span>

    <span class="nd">@rank_zero_only</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Logs Optimizer related metrics when the train begins</span>

<span class="sd">        Args:</span>
<span class="sd">            trainer: pytorch lightning trainer instance</span>
<span class="sd">            pl_module: pytorch lightning base module</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">set_tags</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">run_id</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;Mode&quot;</span><span class="p">:</span> <span class="s2">&quot;training&quot;</span><span class="p">})</span>

        <span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;epochs&quot;</span><span class="p">:</span> <span class="n">trainer</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">}</span>

        <span class="c1"># TODO For logging optimizer params - Following scenarios are to revisited.</span>
        <span class="c1"># 1. In the current scenario, only the first optimizer details are logged.</span>
        <span class="c1">#    Code to be enhanced to log params when multiple optimizers are used.</span>
        <span class="c1"># 2. mlflow.log_params is used to store optimizer default values into mlflow.</span>
        <span class="c1">#    The keys in default dictionary are too short, Ex: (lr - learning_rate).</span>
        <span class="c1">#    Efficient mapping technique needs to be introduced</span>
        <span class="c1">#    to rename the optimizer parameters based on keys in default dictionary.</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="s2">&quot;optimizers&quot;</span><span class="p">):</span>
            <span class="c1"># Lightning &gt;= 1.6.0 increments the global step every time an optimizer is stepped.</span>
            <span class="c1"># We assume every optimizer will be stepped in each training step.</span>
            <span class="k">if</span> <span class="n">_pl_version</span> <span class="o">&gt;=</span> <span class="n">Version</span><span class="p">(</span><span class="s2">&quot;1.6.0&quot;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_global_steps_per_training_step</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">optimizers</span><span class="p">)</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">optimizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">params</span><span class="p">[</span><span class="s2">&quot;optimizer_name&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_get_optimizer_name</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>

            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="s2">&quot;defaults&quot;</span><span class="p">):</span>
                <span class="n">params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">defaults</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">log_params</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">run_id</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">flush</span><span class="p">(</span><span class="n">synchronous</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nd">@rank_zero_only</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Logs the model checkpoint into mlflow - models folder on the training end</span>


<span class="sd">        Args:</span>
<span class="sd">            trainer: pytorch lightning trainer instance</span>
<span class="sd">            pl_module: pytorch lightning base module</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># manually flush any remaining metadata from training</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics_logger</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">flush</span><span class="p">(</span><span class="n">synchronous</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nd">@rank_zero_only</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">on_test_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Logs accuracy and other relevant metrics on the testing end</span>

<span class="sd">        Args:</span>
<span class="sd">            trainer: pytorch lightning trainer instance</span>
<span class="sd">            pl_module: pytorch lightning base module</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">set_tags</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">run_id</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;Mode&quot;</span><span class="p">:</span> <span class="s2">&quot;testing&quot;</span><span class="p">})</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">flush</span><span class="p">(</span><span class="n">synchronous</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">metrics_logger</span><span class="o">.</span><span class="n">record_metrics</span><span class="p">(</span>
            <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">trainer</span><span class="o">.</span><span class="n">callback_metrics</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics_logger</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>


<div class="viewcode-block" id="MlflowModelCheckpointCallback"><a class="viewcode-back" href="../../../python_api/mlflow.pytorch.html#mlflow.pytorch.MlflowModelCheckpointCallback">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">MlflowModelCheckpointCallback</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">Callback</span><span class="p">,</span> <span class="n">MlflowModelCheckpointCallbackBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Callback for auto-logging pytorch-lightning model checkpoints to MLflow.</span>
<span class="sd">    This callback implementation only supports pytorch-lightning &gt;= 1.6.0.</span>

<span class="sd">    Args:</span>
<span class="sd">        monitor: In automatic model checkpointing, the metric name to monitor if</span>
<span class="sd">            you set `model_checkpoint_save_best_only` to True.</span>
<span class="sd">        save_best_only: If True, automatic model checkpointing only saves when</span>
<span class="sd">            the model is considered the &quot;best&quot; model according to the quantity</span>
<span class="sd">            monitored and previous checkpoint model is overwritten.</span>
<span class="sd">        mode: one of {&quot;min&quot;, &quot;max&quot;}. In automatic model checkpointing,</span>
<span class="sd">            if save_best_only=True, the decision to overwrite the current save file is made</span>
<span class="sd">            based on either the maximization or the minimization of the monitored quantity.</span>
<span class="sd">        save_weights_only: In automatic model checkpointing, if True, then</span>
<span class="sd">            only the model&#39;s weights will be saved. Otherwise, the optimizer states,</span>
<span class="sd">            lr-scheduler states, etc are added in the checkpoint too.</span>
<span class="sd">        save_freq: `&quot;epoch&quot;` or integer. When using `&quot;epoch&quot;`, the callback</span>
<span class="sd">            saves the model after each epoch. When using integer, the callback</span>
<span class="sd">            saves the model at end of this many batches. Note that if the saving isn&#39;t</span>
<span class="sd">            aligned to epochs, the monitored metric may potentially be less reliable (it</span>
<span class="sd">            could reflect as little as 1 batch, since the metrics get reset</span>
<span class="sd">            every epoch). Defaults to `&quot;epoch&quot;`.</span>

<span class="sd">    .. code-block:: python</span>
<span class="sd">        :caption: Example</span>

<span class="sd">        import mlflow</span>
<span class="sd">        from mlflow.pytorch import MlflowModelCheckpointCallback</span>
<span class="sd">        from pytorch_lightning import Trainer</span>

<span class="sd">        mlflow.pytorch.autolog(checkpoint=True)</span>

<span class="sd">        model = MyLightningModuleNet()  # A custom-pytorch lightning model</span>
<span class="sd">        train_loader = create_train_dataset_loader()</span>

<span class="sd">        mlflow_checkpoint_callback = MlflowModelCheckpointCallback()</span>

<span class="sd">        trainer = Trainer(callbacks=[mlflow_checkpoint_callback])</span>

<span class="sd">        with mlflow.start_run() as run:</span>
<span class="sd">            trainer.fit(model, train_loader)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
        <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">save_weights_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">save_freq</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">checkpoint_file_suffix</span><span class="o">=</span><span class="s2">&quot;.pth&quot;</span><span class="p">,</span>
            <span class="n">monitor</span><span class="o">=</span><span class="n">monitor</span><span class="p">,</span>
            <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
            <span class="n">save_best_only</span><span class="o">=</span><span class="n">save_best_only</span><span class="p">,</span>
            <span class="n">save_weights_only</span><span class="o">=</span><span class="n">save_weights_only</span><span class="p">,</span>
            <span class="n">save_freq</span><span class="o">=</span><span class="n">save_freq</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="MlflowModelCheckpointCallback.save_checkpoint"><a class="viewcode-back" href="../../../python_api/mlflow.pytorch.html#mlflow.pytorch.MlflowModelCheckpointCallback.save_checkpoint">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filepath</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="c1"># Note: `trainer.save_checkpoint` implementation contains invocation of</span>
        <span class="c1"># `self.strategy.barrier(&quot;Trainer.save_checkpoint&quot;)`,</span>
        <span class="c1"># in DDP training, this callback is only invoked in rank 0 process,</span>
        <span class="c1"># the `barrier` invocation causes deadlock,</span>
        <span class="c1"># so I implement `save_checkpoint` instead of</span>
        <span class="c1"># calling `trainer.save_checkpoint`.</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">_checkpoint_connector</span><span class="o">.</span><span class="n">dump_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">save_weights_only</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">strategy</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">filepath</span><span class="p">)</span></div>

<div class="viewcode-block" id="MlflowModelCheckpointCallback.on_fit_start"><a class="viewcode-back" href="../../../python_api/mlflow.pytorch.html#mlflow.pytorch.MlflowModelCheckpointCallback.on_fit_start">[docs]</a>    <span class="nd">@rank_zero_only</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">on_fit_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="s2">&quot;pl.Trainer&quot;</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">:</span> <span class="s2">&quot;pl.LightningModule&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="o">=</span> <span class="n">trainer</span></div>

<div class="viewcode-block" id="MlflowModelCheckpointCallback.on_train_batch_end"><a class="viewcode-back" href="../../../python_api/mlflow.pytorch.html#mlflow.pytorch.MlflowModelCheckpointCallback.on_train_batch_end">[docs]</a>    <span class="nd">@rank_zero_only</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_batch_end</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">trainer</span><span class="p">:</span> <span class="s2">&quot;pl.Trainer&quot;</span><span class="p">,</span>
        <span class="n">pl_module</span><span class="p">:</span> <span class="s2">&quot;pl.LightningModule&quot;</span><span class="p">,</span>
        <span class="n">outputs</span><span class="p">,</span>
        <span class="n">batch</span><span class="p">,</span>
        <span class="n">batch_idx</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">save_freq</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_freq</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">check_and_save_checkpoint_if_needed</span><span class="p">(</span>
                <span class="n">current_epoch</span><span class="o">=</span><span class="n">trainer</span><span class="o">.</span><span class="n">current_epoch</span><span class="p">,</span>
                <span class="n">global_step</span><span class="o">=</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span><span class="p">,</span>
                <span class="n">metric_dict</span><span class="o">=</span><span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">trainer</span><span class="o">.</span><span class="n">callback_metrics</span><span class="o">.</span><span class="n">items</span><span class="p">()},</span>
            <span class="p">)</span></div>

<div class="viewcode-block" id="MlflowModelCheckpointCallback.on_train_epoch_end"><a class="viewcode-back" href="../../../python_api/mlflow.pytorch.html#mlflow.pytorch.MlflowModelCheckpointCallback.on_train_epoch_end">[docs]</a>    <span class="nd">@rank_zero_only</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="s2">&quot;pl.Trainer&quot;</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">:</span> <span class="s2">&quot;pl.LightningModule&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_freq</span> <span class="o">==</span> <span class="s2">&quot;epoch&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">check_and_save_checkpoint_if_needed</span><span class="p">(</span>
                <span class="n">current_epoch</span><span class="o">=</span><span class="n">trainer</span><span class="o">.</span><span class="n">current_epoch</span><span class="p">,</span>
                <span class="n">global_step</span><span class="o">=</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span><span class="p">,</span>
                <span class="n">metric_dict</span><span class="o">=</span><span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">trainer</span><span class="o">.</span><span class="n">callback_metrics</span><span class="o">.</span><span class="n">items</span><span class="p">()},</span>
            <span class="p">)</span></div></div>


<span class="c1"># PyTorch-Lightning refactored the LoggerConnector class in version 1.4.0 and made metrics</span>
<span class="c1"># update on demand. Prior to this, the metrics from the current step were not available to</span>
<span class="c1"># callbacks immediately, so the view of metrics was off by one step.</span>
<span class="c1"># To avoid this problem, we access the metrics via the logger_connector for older versions.</span>
<span class="k">if</span> <span class="n">_pl_version</span> <span class="o">&gt;=</span> <span class="n">Version</span><span class="p">(</span><span class="s2">&quot;1.4.0&quot;</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_step_metrics</span><span class="p">(</span><span class="n">trainer</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">trainer</span><span class="o">.</span><span class="n">callback_metrics</span>

<span class="k">else</span><span class="p">:</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_step_metrics</span><span class="p">(</span><span class="n">trainer</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">trainer</span><span class="o">.</span><span class="n">logger_connector</span><span class="o">.</span><span class="n">cached_results</span><span class="o">.</span><span class="n">get_latest_batch_log_metrics</span><span class="p">()</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_log_early_stop_params</span><span class="p">(</span><span class="n">early_stop_callback</span><span class="p">,</span> <span class="n">client</span><span class="p">,</span> <span class="n">run_id</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Logs early stopping configuration parameters to MLflow.</span>

<span class="sd">    Args:</span>
<span class="sd">        early_stop_callback: The early stopping callback instance used during training.</span>
<span class="sd">        client: An `MlflowAutologgingQueueingClient` instance used for MLflow logging.</span>
<span class="sd">        run_id: The ID of the MLflow Run to which to log configuration parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">client</span><span class="o">.</span><span class="n">log_params</span><span class="p">(</span>
        <span class="n">run_id</span><span class="p">,</span>
        <span class="p">{</span>
            <span class="n">p</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">early_stop_callback</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;monitor&quot;</span><span class="p">,</span> <span class="s2">&quot;mode&quot;</span><span class="p">,</span> <span class="s2">&quot;patience&quot;</span><span class="p">,</span> <span class="s2">&quot;min_delta&quot;</span><span class="p">,</span> <span class="s2">&quot;stopped_epoch&quot;</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">early_stop_callback</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
        <span class="p">},</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_log_early_stop_metrics</span><span class="p">(</span><span class="n">early_stop_callback</span><span class="p">,</span> <span class="n">client</span><span class="p">,</span> <span class="n">run_id</span><span class="p">,</span> <span class="n">model_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Logs early stopping behavior results (e.g. stopped epoch) as metrics to MLflow.</span>

<span class="sd">    Args:</span>
<span class="sd">        early_stop_callback: The early stopping callback instance used during training.</span>
<span class="sd">        client: An `MlflowAutologgingQueueingClient` instance used for MLflow logging.</span>
<span class="sd">        run_id: The ID of the MLflow Run to which to log configuration parameters.</span>
<span class="sd">        model_id: The ID of the LoggedModel to which the metrics are associated.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">early_stop_callback</span><span class="o">.</span><span class="n">stopped_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;stopped_epoch&quot;</span><span class="p">:</span> <span class="n">early_stop_callback</span><span class="o">.</span><span class="n">stopped_epoch</span><span class="p">,</span>
        <span class="s2">&quot;restored_epoch&quot;</span><span class="p">:</span> <span class="n">early_stop_callback</span><span class="o">.</span><span class="n">stopped_epoch</span> <span class="o">-</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">early_stop_callback</span><span class="o">.</span><span class="n">patience</span><span class="p">),</span>
    <span class="p">}</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">early_stop_callback</span><span class="p">,</span> <span class="s2">&quot;best_score&quot;</span><span class="p">):</span>
        <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;best_score&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">early_stop_callback</span><span class="o">.</span><span class="n">best_score</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">early_stop_callback</span><span class="p">,</span> <span class="s2">&quot;wait_count&quot;</span><span class="p">):</span>
        <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;wait_count&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">early_stop_callback</span><span class="o">.</span><span class="n">wait_count</span>

    <span class="n">client</span><span class="o">.</span><span class="n">log_metrics</span><span class="p">(</span><span class="n">run_id</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">model_id</span><span class="o">=</span><span class="n">model_id</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">patched_fit</span><span class="p">(</span><span class="n">original</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A patched implementation of `pytorch_lightning.Trainer.fit` which enables logging the</span>
<span class="sd">    following parameters, metrics and artifacts:</span>

<span class="sd">    - Training epochs</span>
<span class="sd">    - Optimizer parameters</span>
<span class="sd">    - `EarlyStoppingCallback`_ parameters</span>
<span class="sd">    - Metrics stored in `trainer.callback_metrics`</span>
<span class="sd">    - Model checkpoints</span>
<span class="sd">    - Trained model</span>

<span class="sd">    .. _EarlyStoppingCallback:</span>
<span class="sd">        https://pytorch-lightning.readthedocs.io/en/latest/early_stopping.html</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">MIN_REQ_VERSION</span> <span class="o">&lt;=</span> <span class="n">_pl_version</span> <span class="o">&lt;=</span> <span class="n">MAX_REQ_VERSION</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Autologging is known to be compatible with pytorch-lightning versions between &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">MIN_REQ_VERSION</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">MAX_REQ_VERSION</span><span class="si">}</span><span class="s2"> and may not succeed with packages &quot;</span>
            <span class="s2">&quot;outside this range.&quot;</span>
        <span class="p">)</span>

    <span class="k">with</span> <span class="n">disable_autologging</span><span class="p">():</span>
        <span class="n">run_id</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">active_run</span><span class="p">()</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">run_id</span>
        <span class="n">tracking_uri</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">get_tracking_uri</span><span class="p">()</span>
        <span class="n">client</span> <span class="o">=</span> <span class="n">MlflowAutologgingQueueingClient</span><span class="p">(</span><span class="n">tracking_uri</span><span class="p">)</span>

        <span class="n">log_models</span> <span class="o">=</span> <span class="n">get_autologging_config</span><span class="p">(</span><span class="n">mlflow</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">FLAVOR_NAME</span><span class="p">,</span> <span class="s2">&quot;log_models&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">model_id</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">log_models</span><span class="p">:</span>
            <span class="n">model_id</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">initialize_logged_model</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">model_id</span>
        <span class="n">metrics_logger</span> <span class="o">=</span> <span class="n">BatchMetricsLogger</span><span class="p">(</span><span class="n">run_id</span><span class="p">,</span> <span class="n">tracking_uri</span><span class="p">,</span> <span class="n">model_id</span><span class="o">=</span><span class="n">model_id</span><span class="p">)</span>

        <span class="n">log_every_n_epoch</span> <span class="o">=</span> <span class="n">get_autologging_config</span><span class="p">(</span>
            <span class="n">mlflow</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">FLAVOR_NAME</span><span class="p">,</span> <span class="s2">&quot;log_every_n_epoch&quot;</span><span class="p">,</span> <span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">log_every_n_step</span> <span class="o">=</span> <span class="n">get_autologging_config</span><span class="p">(</span>
            <span class="n">mlflow</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">FLAVOR_NAME</span><span class="p">,</span> <span class="s2">&quot;log_every_n_step&quot;</span><span class="p">,</span> <span class="kc">None</span>
        <span class="p">)</span>

        <span class="n">early_stop_callback</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">callback</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">callback</span><span class="p">,</span> <span class="n">pl</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">early_stopping</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">):</span>
                <span class="n">early_stop_callback</span> <span class="o">=</span> <span class="n">callback</span>
                <span class="n">_log_early_stop_params</span><span class="p">(</span><span class="n">early_stop_callback</span><span class="p">,</span> <span class="n">client</span><span class="p">,</span> <span class="n">run_id</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">__MlflowPLCallback</span><span class="p">)</span> <span class="k">for</span> <span class="n">callbacks</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span> <span class="o">+=</span> <span class="p">[</span>
                <span class="n">__MlflowPLCallback</span><span class="p">(</span>
                    <span class="n">client</span><span class="p">,</span> <span class="n">metrics_logger</span><span class="p">,</span> <span class="n">run_id</span><span class="p">,</span> <span class="n">log_models</span><span class="p">,</span> <span class="n">log_every_n_epoch</span><span class="p">,</span> <span class="n">log_every_n_step</span>
                <span class="p">)</span>
            <span class="p">]</span>

        <span class="n">model_checkpoint</span> <span class="o">=</span> <span class="n">get_autologging_config</span><span class="p">(</span><span class="n">mlflow</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">FLAVOR_NAME</span><span class="p">,</span> <span class="s2">&quot;checkpoint&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model_checkpoint</span><span class="p">:</span>
            <span class="c1"># __MLflowModelCheckpoint only supports pytorch-lightning &gt;= 1.6.0</span>
            <span class="k">if</span> <span class="n">_pl_version</span> <span class="o">&gt;=</span> <span class="n">Version</span><span class="p">(</span><span class="s2">&quot;1.6.0&quot;</span><span class="p">):</span>
                <span class="n">checkpoint_monitor</span> <span class="o">=</span> <span class="n">get_autologging_config</span><span class="p">(</span>
                    <span class="n">mlflow</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">FLAVOR_NAME</span><span class="p">,</span> <span class="s2">&quot;checkpoint_monitor&quot;</span><span class="p">,</span> <span class="s2">&quot;val_loss&quot;</span>
                <span class="p">)</span>
                <span class="n">checkpoint_mode</span> <span class="o">=</span> <span class="n">get_autologging_config</span><span class="p">(</span>
                    <span class="n">mlflow</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">FLAVOR_NAME</span><span class="p">,</span> <span class="s2">&quot;checkpoint_mode&quot;</span><span class="p">,</span> <span class="s2">&quot;min&quot;</span>
                <span class="p">)</span>
                <span class="n">checkpoint_save_best_only</span> <span class="o">=</span> <span class="n">get_autologging_config</span><span class="p">(</span>
                    <span class="n">mlflow</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">FLAVOR_NAME</span><span class="p">,</span> <span class="s2">&quot;checkpoint_save_best_only&quot;</span><span class="p">,</span> <span class="kc">True</span>
                <span class="p">)</span>
                <span class="n">checkpoint_save_weights_only</span> <span class="o">=</span> <span class="n">get_autologging_config</span><span class="p">(</span>
                    <span class="n">mlflow</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">FLAVOR_NAME</span><span class="p">,</span> <span class="s2">&quot;checkpoint_save_weights_only&quot;</span><span class="p">,</span> <span class="kc">False</span>
                <span class="p">)</span>
                <span class="n">checkpoint_save_freq</span> <span class="o">=</span> <span class="n">get_autologging_config</span><span class="p">(</span>
                    <span class="n">mlflow</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">FLAVOR_NAME</span><span class="p">,</span> <span class="s2">&quot;checkpoint_save_freq&quot;</span><span class="p">,</span> <span class="s2">&quot;epoch&quot;</span>
                <span class="p">)</span>

                <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
                    <span class="nb">isinstance</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">MlflowModelCheckpointCallback</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">callbacks</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span>
                <span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span> <span class="o">+=</span> <span class="p">[</span>
                        <span class="n">MlflowModelCheckpointCallback</span><span class="p">(</span>
                            <span class="n">monitor</span><span class="o">=</span><span class="n">checkpoint_monitor</span><span class="p">,</span>
                            <span class="n">mode</span><span class="o">=</span><span class="n">checkpoint_mode</span><span class="p">,</span>
                            <span class="n">save_best_only</span><span class="o">=</span><span class="n">checkpoint_save_best_only</span><span class="p">,</span>
                            <span class="n">save_weights_only</span><span class="o">=</span><span class="n">checkpoint_save_weights_only</span><span class="p">,</span>
                            <span class="n">save_freq</span><span class="o">=</span><span class="n">checkpoint_save_freq</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;Automatic model checkpointing is disabled because this feature only &quot;</span>
                    <span class="s2">&quot;supports pytorch-lightning &gt;= 1.6.0.&quot;</span>
                <span class="p">)</span>

        <span class="n">client</span><span class="o">.</span><span class="n">flush</span><span class="p">(</span><span class="n">synchronous</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">result</span> <span class="o">=</span> <span class="n">original</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">early_stop_callback</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_log_early_stop_metrics</span><span class="p">(</span><span class="n">early_stop_callback</span><span class="p">,</span> <span class="n">client</span><span class="p">,</span> <span class="n">run_id</span><span class="p">,</span> <span class="n">model_id</span><span class="o">=</span><span class="n">model_id</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">Version</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">Version</span><span class="p">(</span><span class="s2">&quot;1.4.0&quot;</span><span class="p">):</span>
            <span class="n">summary</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">ModelSummary</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">summary</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">ModelSummary</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">TemporaryDirectory</span><span class="p">()</span> <span class="k">as</span> <span class="n">tempdir</span><span class="p">:</span>
            <span class="n">summary_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempdir</span><span class="p">,</span> <span class="s2">&quot;model_summary.txt&quot;</span><span class="p">)</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">summary_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">summary</span><span class="p">)</span>

            <span class="n">mlflow</span><span class="o">.</span><span class="n">log_artifact</span><span class="p">(</span><span class="n">local_path</span><span class="o">=</span><span class="n">summary_file</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">log_models</span><span class="p">:</span>
            <span class="n">registered_model_name</span> <span class="o">=</span> <span class="n">get_autologging_config</span><span class="p">(</span>
                <span class="n">mlflow</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">FLAVOR_NAME</span><span class="p">,</span> <span class="s2">&quot;registered_model_name&quot;</span><span class="p">,</span> <span class="kc">None</span>
            <span class="p">)</span>
            <span class="n">mlflow</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
                <span class="n">registered_model_name</span><span class="o">=</span><span class="n">registered_model_name</span><span class="p">,</span>
                <span class="n">model_id</span><span class="o">=</span><span class="n">model_id</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">early_stop_callback</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_callback</span><span class="o">.</span><span class="n">best_model_path</span><span class="p">:</span>
                <span class="n">mlflow</span><span class="o">.</span><span class="n">log_artifact</span><span class="p">(</span>
                    <span class="n">local_path</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_callback</span><span class="o">.</span><span class="n">best_model_path</span><span class="p">,</span>
                    <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;restored_model_checkpoint&quot;</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="n">client</span><span class="o">.</span><span class="n">flush</span><span class="p">(</span><span class="n">synchronous</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">result</span>
</pre></div>

              </div>
            </div>
            <footer>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../../../',
      VERSION:'3.1.1.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../../../_static/clippy.svg";</script>
  <script type="text/javascript" src="../../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>